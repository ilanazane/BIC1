{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "salicon_SNN_saliency.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtqVvuCC0_js",
        "outputId": "5320611e-30d3-4189-d760-51db0c7571c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG25iC3VIx0E",
        "outputId": "bf4f0354-9d36-40d3-bab4-72085400c9b7"
      },
      "source": [
        "%cd '/content/drive/My Drive/salicon'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1miA8rtqTtMBSOcF7WVQnBjGRbn32yDoB/salicon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuFvBRFlI1Qx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e62ca81-b808-4941-d804-8c4eaafe100c"
      },
      "source": [
        "!mkdir 'mlnet'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘mlnet’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePk5Tzvr1Qs9",
        "outputId": "063bf927-e233-4435-c29a-75f5cb678d8e"
      },
      "source": [
        "%cd '/content/drive/My Drive/salicon/mlnet'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1miA8rtqTtMBSOcF7WVQnBjGRbn32yDoB/salicon/mlnet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqzJAUOGocqN"
      },
      "source": [
        "## 0. Downloading Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4UoAkVmKq0M"
      },
      "source": [
        "# ! pip install gdown"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsIXGSW4iCDe"
      },
      "source": [
        "# !gdown https://drive.google.com/uc?id=1g8j-hTT-51IG1UFwP0xTGhLdgIUCW5e5&export=download\n",
        "# ! unzip images.zip\n",
        "# !gdown https://drive.google.com/a/umn.edu/uc?id=1PnO7szbdub1559LfjYHMy65EDC4VhJC8&export=download\n",
        "# ! unzip maps.zip  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjKlD3pGHxYw"
      },
      "source": [
        "Manually do this step\n",
        "\n",
        "\n",
        "\n",
        "1. Train Images -> images\n",
        "2. Test Images -> test_images\n",
        "3. Val Images -> val_images\n",
        "4. Train maps -> train\n",
        "5. Val maps -> val\n",
        "\n",
        "\n",
        "#### Sometimes there will be error related to the data, it is because your drive is slow. Reload again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmA5aTMf2Bgh",
        "outputId": "764c94a8-9f80-416e-9f64-79a0cba6e08c"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.png   14.png  19.png  23.png  28.png  6.png        \u001b[0m\u001b[01;34mimages\u001b[0m/        \u001b[01;34mtest_maps\u001b[0m/\n",
            "10.png  15.png  1.png   24.png  2.png   7.png        images.zip     \u001b[01;34mtrain\u001b[0m/\n",
            "11.png  16.png  20.png  25.png  3.png   8.png        maps.zip       \u001b[01;34mval\u001b[0m/\n",
            "12.png  17.png  21.png  26.png  4.png   9.png        \u001b[01;34msaved_models\u001b[0m/  \u001b[01;34mval_images\u001b[0m/\n",
            "13.png  18.png  22.png  27.png  5.png   \u001b[01;34mCONV_PRIOR\u001b[0m/  \u001b[01;34mtest_images\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIxmTdqXn6fZ"
      },
      "source": [
        "import cv2\n",
        "import numpy as np \n",
        "import os \n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "imgs_train_path = 'images/'\n",
        "maps_train_path = 'train/'\n",
        "\n",
        "imgs_val_path = 'val_images/'\n",
        "maps_val_path = 'val/'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6Q3zsRZRDn3"
      },
      "source": [
        "\n",
        "def padding(img, shape_r=480, shape_c=640, channels=3):\n",
        "    if channels == 1:\n",
        "        img_padded = np.zeros((shape_r, shape_c), dtype=np.uint8)\n",
        "    else:\n",
        "      img_padded = np.zeros((shape_r, shape_c, channels), dtype=np.uint8)\n",
        "\n",
        "    original_shape = img.shape\n",
        "    rows_rate = original_shape[0]/shape_r\n",
        "    cols_rate = original_shape[1]/shape_c\n",
        "\n",
        "    if rows_rate > cols_rate:\n",
        "        new_cols = (original_shape[1] * shape_r) // original_shape[0]\n",
        "        img = cv2.resize(img, (new_cols, shape_r))\n",
        "        if new_cols > shape_c:\n",
        "            new_cols = shape_c\n",
        "        img_padded[:, ((img_padded.shape[1] - new_cols) // 2):((img_padded.shape[1] - new_cols) // 2 + new_cols)] = img\n",
        "    else:\n",
        "        new_rows = (original_shape[0] * shape_c) // original_shape[1]\n",
        "        img = cv2.resize(img, (shape_c, new_rows))\n",
        "        if new_rows > shape_r:\n",
        "            new_rows = shape_r\n",
        "        img_padded[((img_padded.shape[0] - new_rows) // 2):((img_padded.shape[0] - new_rows) // 2 + new_rows), :] = img\n",
        "\n",
        "    return img_padded\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9hiQzuCREPl"
      },
      "source": [
        "\n",
        "\n",
        "def preprocess_images(paths, shape_r, shape_c):\n",
        "    ims = np.zeros((len(paths), shape_r, shape_c, 3))\n",
        "\n",
        "    for i, path in enumerate(paths):\n",
        "        original_image = cv2.imread(path)\n",
        "        padded_image = padding(original_image, shape_r, shape_c, 3)\n",
        "        ims[i] = padded_image.astype('float')\n",
        "    \n",
        "    ims = ims[...,::-1]\n",
        "    ims /= 255.0\n",
        "    ims = np.rollaxis(ims, 3, 1)  \n",
        "    return ims\n",
        "\n",
        "\n",
        "def preprocess_maps(paths, shape_r, shape_c):\n",
        "    ims = np.zeros((len(paths), 1, shape_r, shape_c))\n",
        "\n",
        "    for i, path in enumerate(paths):\n",
        "        original_map = cv2.imread(path, 0)\n",
        "        padded_map = padding(original_map, shape_r, shape_c, 1)\n",
        "        ims[i, 0] = padded_map.astype(np.float32)\n",
        "        ims[i, 0] /= 255.0\n",
        "        \n",
        "\n",
        "    return ims"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Td9bdophNQs"
      },
      "source": [
        "def generator(b_s, phase_gen='train',th = False):\n",
        "    if phase_gen == 'train':\n",
        "        images = [imgs_train_path + f for f in os.listdir(imgs_train_path) if f.endswith('.jpg')]\n",
        "        maps = [maps_train_path + f for f in os.listdir(maps_train_path) if f.endswith('.png')]\n",
        "        if th == True:\n",
        "          model.module.network_update(timesteps=100, leak=1.0)\n",
        "    elif phase_gen == 'val':\n",
        "        images = [imgs_val_path + f for f in os.listdir(imgs_val_path) if f.endswith('.jpg')]\n",
        "        maps = [maps_val_path + f for f in os.listdir(maps_val_path) if f.endswith('.png')]\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    images.sort()\n",
        "    maps.sort()\n",
        "    counter = 0\n",
        "    \n",
        "    while True:\n",
        "        yield preprocess_images(images[counter:counter + b_s], shape_r, shape_c), preprocess_maps(maps[counter:counter + b_s], shape_r_gt, shape_c_gt)\n",
        "        if counter + b_s >= len(images):\n",
        "          break\n",
        "        counter = counter + b_s"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsPK2TbLHF1d"
      },
      "source": [
        "class PoissonGenerator(nn.Module):\n",
        "  def _init_(self):\n",
        "    super().__init__()\n",
        "  \n",
        "  def forward(self,input):\n",
        "   out = torch.mul(torch.le(torch.rand_like(input), torch.abs(input)*1.0).float(),torch.sign(input))\n",
        "   return out\n",
        "\t\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXeN0W1NdWpK"
      },
      "source": [
        "class STDB(torch.autograd.Function):\n",
        "\n",
        "\talpha \t= ''\n",
        "\tbeta \t= ''\n",
        "    \n",
        "\t@staticmethod\n",
        "\tdef forward(ctx, input, last_spike):\n",
        "        \n",
        "\t\tctx.save_for_backward(last_spike)\n",
        "\t\tout = torch.zeros_like(input).cuda()\n",
        "\t\tout[input > 0] = 1.0\n",
        "\t\treturn out\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef backward(ctx, grad_output):\n",
        "\t    \t\t\n",
        "\t\tlast_spike, = ctx.saved_tensors\n",
        "\t\tgrad_input = grad_output.clone()\n",
        "\t\tgrad = STDB.alpha * torch.exp(-1*last_spike)**STDB.beta\n",
        "\t\treturn grad*grad_input, None\n",
        "\n",
        "class LinearSpike(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Here we use the piecewise-linear surrogate gradient as was done\n",
        "    in Bellec et al. (2018).\n",
        "    \"\"\"\n",
        "    gamma = 0.3 # Controls the dampening of the piecewise-linear surrogate gradient\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, last_spike):\n",
        "        \n",
        "        ctx.save_for_backward(input)\n",
        "        out = torch.zeros_like(input).cuda()\n",
        "        out[input > 0] = 1.0\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \n",
        "        input,     = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad       = LinearSpike.gamma*F.threshold(1.0-torch.abs(input), 0, 0)\n",
        "        return grad*grad_input, None"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyEzSPPrVcjk"
      },
      "source": [
        "# test = list(models.vgg16(pretrained=True).features)\n",
        "# print(test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34fvj710WDS9"
      },
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision.models as models\n",
        "# # pool of square window of size=3, stride=2\n",
        "#m = nn.MaxPool2d(kernel_size=5, stride=1,padding=2, dilation=1, ceil_mode=False)\n",
        "#m = nn.AvgPool2d(kernel_size=5, stride=1,padding=2)\n",
        "# pool of non-square window\n",
        "# #m = nn.AvgPool2d((3, 2), stride=(2, 1))\n",
        "# input = torch.randn(16, 640, 30, 40)\n",
        "# output = m(input)\n",
        "# print(output.shape)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYQDzgEB9ZY3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pdb\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "from matplotlib import pyplot as plt\n",
        "import copy\n",
        "\n",
        "\n",
        "\n",
        "class VGG_SNN_STDB(nn.Module):\n",
        "  def __init__(self,activation='Linear',timesteps=100,leak=1.0,default_threshold=1.0,alpha=0.3,beta=0.01,dropout=0.2,kernel_size=3):\n",
        "    super().__init__()\n",
        "    if activation == 'Linear':\n",
        "      self.act_func = LinearSpike.apply\n",
        "    elif activation == 'STDB':\n",
        "      self.act_func = STDB.apply\n",
        "    \n",
        "    self.timesteps = timesteps\n",
        "    self.leak = torch.tensor(leak)\n",
        "    STDB.alpha = alpha\n",
        "    STDB.beta = beta\n",
        "    self.dropout = dropout\n",
        "    self.kernel_size = kernel_size\n",
        "    self.input_layer \t= PoissonGenerator().cuda()\n",
        "    self.threshold \t\t= {}\n",
        "    self.mem \t\t\t= {}\n",
        "    self.mask \t\t\t= {}\n",
        "    self.spike \t\t\t= {}\n",
        "    self.cfg = [64, 64, 'A', 128, 128, 'A', 256, 256, 256, 'A', 512, 512, 512, 'AR', 512, 512, 512, 'E']\n",
        "    self.features = self._make_layers()\n",
        "\n",
        "\n",
        "    self._initialize_weights2()\n",
        "\n",
        "    for l in range(len(self.features)):\n",
        "      if isinstance(self.features[l], nn.Conv2d):\n",
        "        self.threshold[l] = torch.tensor(default_threshold)\n",
        "\n",
        "  def _initialize_weights2(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        if m.bias is not None:\n",
        "          m.bias.data.zero_()\n",
        "  \n",
        "  def threshold_update(self, scaling_factor=1.0, thresholds=[]):\n",
        "    self.scaling_factor = scaling_factor\n",
        "\n",
        "    for pos in range(len(self.features)):\n",
        "      if isinstance(self.features[pos], nn.Conv2d):\n",
        "        if thresholds:\n",
        "          self.threshold[pos] = torch.tensor(thresholds.pop(0)*self.scaling_factor)\n",
        "  \n",
        "  def _make_layers(self):\n",
        "    in_channels = 3\n",
        "\n",
        "    layers = []\n",
        "\n",
        "    for x in (self.cfg):\n",
        "      stride = 1\n",
        "\n",
        "      if x == 'A':\n",
        "        \n",
        "        #layers.pop()\n",
        "        layers += [nn.AvgPool2d(kernel_size=2, stride=2,padding=0)]\n",
        "      elif x == 'AR':\n",
        "        \n",
        "        #layers.pop()\n",
        "        layers += [nn.AvgPool2d(kernel_size=5, stride=1, padding=2)]\n",
        "      elif x == 'E':\n",
        "    \n",
        "        #layers.pop()\n",
        "        layers += [nn.Dropout(self.dropout),nn.Conv2d(512,64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),bias=False),\n",
        "                nn.Conv2d(64,1,kernel_size=(1, 1), stride=(1, 1) ,padding=(0, 0),bias = False),\n",
        "                nn.ReLU(inplace=True)]\n",
        "      else:\n",
        "       \n",
        "        layers += [nn.Conv2d(in_channels,x,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),bias=False),\n",
        "                   nn.ReLU(inplace=True)]\n",
        "  \n",
        "        \n",
        "          \n",
        "    \n",
        "        in_channels = x\n",
        "\n",
        "\n",
        "    features = nn.Sequential(*layers)\n",
        "\n",
        "    return features\n",
        "\n",
        "  def network_update(self, timesteps, leak):\n",
        "    self.timesteps = timesteps\n",
        "    self.leak = torch.tensor(leak)\n",
        "\n",
        "  def neuron_init(self, x):\n",
        "    self.batch_size = x.size(0)\n",
        "    self.width = x.size(2)\n",
        "    self.height = x.size(3)\n",
        "  \n",
        "\n",
        "    for l in range(len(self.features)):\n",
        "      if isinstance(self.features[l], nn.Conv2d):\n",
        "        self.mem[l] = torch.zeros(self.batch_size,self.features[l].out_channels,self.width,self.height)\n",
        "\n",
        "        #self.spike[l] = torch.zeros(self.batch_size,self.features[l].out_channels,self.width,self.height)\n",
        "\n",
        "      elif isinstance(self.features[l], nn.Dropout):\n",
        "        self.mask[l] = self.features[l](torch.ones(self.mem[l-2].shape))\n",
        "\n",
        "      \n",
        "      elif isinstance(self.features[l], nn.AvgPool2d):\n",
        "        self.width = ((self.width +2*self.features[l].padding - self.features[l].kernel_size)//self.features[l].stride) + 1\n",
        "        self.height = ((self.height +2*self.features[l].padding - self.features[l].kernel_size)//self.features[l].stride) + 1\n",
        "\n",
        "\n",
        "\n",
        "    self.spike = copy.deepcopy(self.mem)\n",
        "\n",
        "    for key,values in self.spike.items():\n",
        "      for value in values:\n",
        "        value.fill_(-1000)\n",
        "  \n",
        "  def forward(self,x,find_max_mem=False,max_mem_layer=0):\n",
        "    self.neuron_init(x)\n",
        "    max_mem=0.0\n",
        "\n",
        "    for t in range(self.timesteps):\n",
        "      out_prev = self.input_layer(x)\n",
        " \n",
        "\n",
        "      for l in range(len(self.features)):\n",
        "        if isinstance(self.features[l],(nn.Conv2d)):\n",
        "          if find_max_mem and l==max_mem_layer:\n",
        "            if (self.features[l](out_prev)).max()>max_mem:\n",
        "              max_mem = (self.features[l](out_prev)).max()\n",
        "              break\n",
        "\n",
        "          mem_thr = (self.mem[l].cuda()/self.threshold[l].cuda())- 1.0\n",
        "          out = self.act_func(mem_thr, (t-1-self.spike[l].cuda()))\n",
        "          rst = self.threshold[l].cuda()*(mem_thr>0).float()\n",
        "          self.spike[l] = self.spike[l].cuda().masked_fill(out.bool(),t-1)\n",
        "          self.mem[l] = self.leak*self.mem[l].cuda()+self.features[l](out_prev)-rst\n",
        "          out_prev = out.clone()\n",
        "        elif isinstance(self.features[l], nn.AvgPool2d):\n",
        "          out_prev = self.features[l](out_prev)\n",
        "        \n",
        "        elif isinstance(self.features[l], nn.Dropout):\n",
        "          out_prev = out_prev*self.mask[l].cuda()\n",
        "\n",
        "      if find_max_mem and max_mem_layer<len(self.features):\n",
        "        continue\n",
        "      prev = len(self.features)\n",
        "\n",
        "      if not find_max_mem:\n",
        "        self.mem[prev+1] = out_prev\n",
        "   \n",
        "    if find_max_mem:\n",
        "      return max_mem\n",
        "   \n",
        "    return self.mem[prev+1]\n",
        "    \n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1-vcw1so-m6",
        "outputId": "d77fb40a-882e-4c38-d407-b177a9fc958d"
      },
      "source": [
        "!pip3 install torchvision"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n",
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.7.0+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GojPzAz3Lnxo"
      },
      "source": [
        "### Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrzCqMe3pyFM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "def find_threshold(batch_size=512, timesteps=2500, architecture='VGG16'):\n",
        "    \n",
        "    model.module.network_update(timesteps=timesteps, leak=1.0)\n",
        "\n",
        "    pos=0\n",
        "    thresholds=[]\n",
        "    \n",
        "    def find(layer, pos):\n",
        "        max_act=0\n",
        "        trained = 0\n",
        "        \n",
        "        for i,gt_map in generator(batch_size,th=False):\n",
        "        #for batch_idx, (data, target) in enumerate(loader):\n",
        "            i,gt_map = torch.tensor(i.copy(),dtype=torch.float),torch.tensor(gt_map,dtype=torch.float)\n",
        "            data, target = i.cuda(), gt_map.cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                output = model(data, find_max_mem=True, max_mem_layer=layer)\n",
        "                if output>max_act:\n",
        "                    max_act = output.item()\n",
        "\n",
        "                #f.write('\\nBatch:{} Current:{:.4f} Max:{:.4f}'.format(batch_idx+1,output.item(),max_act))\n",
        "                if i.shape[0] ==0:\n",
        "                    thresholds.append(max_act)\n",
        "                    pos = pos+1\n",
        "                    #f.write(' {}'.format(thresholds))\n",
        "                    model.module.threshold_update(scaling_factor=1.0, thresholds=thresholds[:])\n",
        "                    break\n",
        "            trained = trained + i.shape[0]\n",
        "            if trained % (i.shape[0]) == 0:\n",
        "              print (\"Findthresholds:{}\".format(trained))\n",
        "        return pos\n",
        "\n",
        "    if architecture.lower().startswith('vgg'):              \n",
        "        for l in model.module.features.named_children():\n",
        "            if isinstance(l[1], nn.Conv2d):\n",
        "                pos = find(int(l[0]), pos)\n",
        "        \n",
        "\n",
        "\n",
        "    return thresholds\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxuCG8B6MpEd"
      },
      "source": [
        "### Custom Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXEtwMekX9Ov"
      },
      "source": [
        "## With Prior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZHl2DIX5Jj"
      },
      "source": [
        "# Modified MSE Loss Function\n",
        "class ModMSELoss(torch.nn.Module):\n",
        "    def __init__(self,shape_r_gt,shape_c_gt):\n",
        "        super(ModMSELoss, self).__init__()\n",
        "        self.shape_r_gt = shape_r_gt\n",
        "        self.shape_c_gt = shape_c_gt\n",
        "        \n",
        "    def forward(self, output , label):\n",
        "        prior_size = prior.shape\n",
        "        output_max = torch.max(torch.max(output,2)[0],2)[0].unsqueeze(2).unsqueeze(2).expand(output.shape[0],output.shape[1],self.shape_r_gt,self.shape_c_gt)\n",
        "        #reg = ( 1.0/(prior_size[0]*prior_size[1]) ) * ( 1 - prior)**2\n",
        "        loss = torch.mean( ((output / output_max) - label)**2 / (1 - label + 0.1) ) # +  torch.sum(reg)\n",
        "        return loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L67bssepBflY"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsnFIGwPRJ4X"
      },
      "source": [
        "shape_r = 240\n",
        "shape_c = 320\n",
        "shape_r_gt = 30\n",
        "shape_c_gt = 40\n",
        "last_freeze_layer = 23"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qbZC28MdPK1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8031c32f-fd52-4896-d498-77158aad4e99"
      },
      "source": [
        "#model0 = MLNet(prior_size).cuda()\n",
        "# give the path where model is stored\n",
        "cur_list = []\n",
        "model = VGG_SNN_STDB(activation = 'Linear', timesteps=100, leak=1.0, default_threshold=1.0, alpha=0.3, beta=0.01, dropout=0.3, kernel_size=3).cuda()\n",
        "model = nn.DataParallel(model)\n",
        "                                 \n",
        "PATH = 'saved_models/Copy of 2020-12-09 noconcat_noprior.model'   # path of created model\n",
        "state = torch.load(PATH)\n",
        "# print(\"len: \",len(state))\n",
        "# for key in state:\n",
        "#   print(\"state: \", key)\n",
        "cur_dict = model.state_dict() \n",
        "for key in cur_dict:\n",
        "  cur_list.append(key)\n",
        "  #print(\"cur key: \",key)\n",
        "\n",
        "#print(len(cur_list))\n",
        "\n",
        "#input(\"Enter\")\n",
        "count = 0\n",
        "cur_count = 0\n",
        "for key in state:\n",
        "  if count < 34:\n",
        "    if count%2==0:\n",
        "      cur_dict[cur_list[cur_count]] = nn.Parameter(state[key].data)\n",
        "      cur_count +=1\n",
        "\n",
        "      #f.write('\\n Success: Loaded {} from {}'.format(key, path))\n",
        "      #print(\"succcess!\")\n",
        "    # else:\n",
        "    #   print(\"mismatch\")\n",
        "    #   #f.write('\\n Error: Size mismatch, size of loaded model {}, size of current model {}'.format(state['state_dict'][key].shape, model.state_dict()[key].shape))\n",
        "    count += 1\n",
        "\n",
        "\n",
        "# for key in cur_dict:\n",
        "#   print(\"now cur key: \",key)\n",
        "# input(\"enter\")\n",
        "\n",
        "\n",
        "\n",
        "model.load_state_dict(cur_dict)\n",
        "#input(\"enter\")\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XVNuAfPK41f",
        "outputId": "4355e6e8-abc4-4bdc-d8ff-f287f4cc2473"
      },
      "source": [
        "if 'thresholds' in state:\n",
        "  thresholds = state['thresholds']\n",
        "  f.write('\\n Info: Thresholds loaded from trained ANN: {}'.format(thresholds))\n",
        "  model.module.threshold_update(scaling_factor = scaling_factor, thresholds=thresholds[:])\n",
        "else:\n",
        "  thresholds = find_threshold(batch_size=32, timesteps=1000, architecture='VGG16')\n",
        "  model.module.threshold_update(scaling_factor=0.7,thresholds = thresholds[:])\n",
        "  temp = {}\n",
        "  for key, value in state.items():\n",
        "    temp[key] = value\n",
        "  temp['thresholds'] = thresholds\n",
        "  torch.save(thresholds, 'saved_models/thresholds.pt')\n",
        "  torch.save(temp,PATH)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Findthresholds:32\n",
            "Findthresholds:64\n",
            "Findthresholds:96\n",
            "Findthresholds:128\n",
            "Findthresholds:160\n",
            "Findthresholds:192\n",
            "Findthresholds:224\n",
            "Findthresholds:256\n",
            "Findthresholds:288\n",
            "Findthresholds:320\n",
            "Findthresholds:352\n",
            "Findthresholds:384\n",
            "Findthresholds:416\n",
            "Findthresholds:448\n",
            "Findthresholds:480\n",
            "Findthresholds:512\n",
            "Findthresholds:544\n",
            "Findthresholds:576\n",
            "Findthresholds:608\n",
            "Findthresholds:640\n",
            "Findthresholds:672\n",
            "Findthresholds:704\n",
            "Findthresholds:736\n",
            "Findthresholds:768\n",
            "Findthresholds:800\n",
            "Findthresholds:832\n",
            "Findthresholds:864\n",
            "Findthresholds:896\n",
            "Findthresholds:928\n",
            "Findthresholds:960\n",
            "Findthresholds:992\n",
            "Findthresholds:1024\n",
            "Findthresholds:1056\n",
            "Findthresholds:1088\n",
            "Findthresholds:1120\n",
            "Findthresholds:1152\n",
            "Findthresholds:1184\n",
            "Findthresholds:1216\n",
            "Findthresholds:1248\n",
            "Findthresholds:1280\n",
            "Findthresholds:1312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4wrJnCdWsQJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "ab642fae-2351-429b-bf0d-cc7d51fa85f9"
      },
      "source": [
        "\n",
        "criterion = ModMSELoss(shape_r_gt,shape_c_gt).cuda()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3,weight_decay=0.0005,momentum=0.9,nesterov=True)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4,weight_decay=5e-4)\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "loss_history = []\n",
        "nb_epochs = 10\n",
        "batch_size = 8\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "  t1 = time.time()\n",
        "  image_trained = 0\n",
        "  for i,gt_map in generator(batch_size,th=True):\n",
        "      optimizer.zero_grad()\n",
        "      i,gt_map = torch.tensor(i.copy(),dtype=torch.float),torch.tensor(gt_map,dtype=torch.float)\n",
        "      for idx,x in enumerate(i):\n",
        "        i[idx] = normalize(x)   \n",
        "      i,gt_map = i.cuda(),gt_map.cuda()\n",
        "      image_trained += batch_size\n",
        "      out = model.forward(i)\n",
        "      loss = criterion(out,gt_map)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "      if image_trained % (batch_size*20) == 0:\n",
        "        print (\"Epochs:{} Images:{} Loss:{}\".format(epoch,image_trained,loss.item()) )\n",
        "  t2 = time.time()\n",
        "  time_per_epoch = (t2 - t1) / 60.0\n",
        "  print ('Time taken for epoch-{} : {}m'.format(epoch,time_per_epoch))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e25180e52c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgt_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgt_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mimage_trained\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgt_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-9bd1995da136>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, find_max_mem, max_mem_layer)\u001b[0m\n\u001b[1;32m    144\u001b[0m           \u001b[0mrst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_thr\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleak\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m           \u001b[0mout_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAvgPool2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 15.75 GiB total capacity; 14.11 GiB already allocated; 58.88 MiB free; 14.48 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHzVTbJsQteV"
      },
      "source": [
        "## savind model\n",
        "\n",
        "!mkdir saved_models -p\n",
        "additional_info = 'my_model'\n",
        "full_path = 'saved_models/' + str(datetime.datetime.now()) + '_' + additional_info + '.model'\n",
        "torch.save(model.state_dict(), full_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgeWfu5yBnBU"
      },
      "source": [
        "## Model Output "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMhGnQ8p6P7n"
      },
      "source": [
        "# give the path where model is stored\n",
        "PATH = '2020-12-09 noconcat_noprior.model'   # path of created model\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg9o2xPNzwEf"
      },
      "source": [
        "num = 30\n",
        "cnt = 0\n",
        "for i,gt_map in generator(1,phase_gen='val',th=True):\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    org_image = i[0].copy()\n",
        "    org_image = np.rollaxis(org_image, 0, 3) \n",
        "    gt_map = torch.tensor(gt_map,dtype=torch.float)\n",
        "    gt_map = model1.input_layer(gt_map)\n",
        "    i = torch.tensor(i.copy(),dtype=torch.float)\n",
        "    for idx,x in enumerate(i):\n",
        "        i[idx] = normalize(x)\n",
        "    i = i.cuda()\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    ax1.set_title('original')\n",
        "    plt.imshow(org_image)\n",
        "    # ax1.plot(org_image)\n",
        "    out0 = model0.forward(i)\n",
        "    out = model1.forward(out0)\n",
        "    ax1 = plt.subplot(3, 3, 2)\n",
        "    ax1.set_title('predicted')\n",
        "    plt.imshow(out[0].squeeze(0).data.cpu().numpy(),cmap='gray')\n",
        "\n",
        "    ax1 = plt.subplot(3, 3, 3)\n",
        "    ax1.set_title('gt')\n",
        "    plt.imshow(gt_map[0][0].data,cmap='gray')\n",
        "    \n",
        "    plt.show()\n",
        "    fig.savefig('%s.png'%cnt)\n",
        "    cnt += 1\n",
        "    if cnt > num:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_U3twF2UIqp"
      },
      "source": [
        "# #### comparing 2 models  ## No need to run This\n",
        "\n",
        "# PATH1 = 'saved_models/2019-11-22 07:24:38.815721_my_model.model'\n",
        "# PATH2 = 'saved_models2/2019-11-21 10_43_21.727494_my_model.model'\n",
        "# model1 = MLNet(prior_size).cuda()\n",
        "# model2 = MLNet(prior_size).cuda()\n",
        "\n",
        "# model1.load_state_dict(torch.load(PATH1))\n",
        "# model2.load_state_dict(torch.load(PATH2))\n",
        "\n",
        "\n",
        "# cnt = 0\n",
        "# for i,gt_map in generator(1,phase_gen='val'):\n",
        "#     fig = plt.figure(figsize=(20, 20))\n",
        "#     org_image = i[0].copy()\n",
        "#     org_image = np.rollaxis(org_image, 0, 3) \n",
        "#     i = torch.tensor(i.copy(),dtype=torch.float)\n",
        "#     for idx,x in enumerate(i):\n",
        "#         i[idx] = normalize(x)\n",
        "#     i = i.cuda()\n",
        "#     ax1 = plt.subplot(4, 4, 1)\n",
        "#     ax1.set_title('original')\n",
        "#     plt.imshow(org_image)\n",
        "    \n",
        "#     out1 = model1.forward(i)\n",
        "#     out2 = model2.forward(i)\n",
        "    \n",
        "#     ax1 = plt.subplot(4, 4, 2)\n",
        "#     ax1.set_title('predicted no prior')\n",
        "#     plt.imshow(out1[0].squeeze(0).data.cpu().numpy(),cmap='gray')\n",
        "#     ax1 = plt.subplot(4, 4, 3)\n",
        "#     ax1.set_title('predicted no Prior')\n",
        "#     plt.imshow(out2[0].squeeze(0).data.cpu().numpy(),cmap='gray')\n",
        "\n",
        "#     ax1 = plt.subplot(4, 4, 4)\n",
        "#     ax1.set_title('gt')\n",
        "#     plt.imshow(gt_map[0][0],cmap='gray')\n",
        "    \n",
        "#     plt.show()\n",
        "#     fig.savefig('%s.png'%visual_cnt)\n",
        "#     cnt += 1\n",
        "#     if cnt > num:\n",
        "#       break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzp1rQ9tBxsd"
      },
      "source": [
        "## Run on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CspRk5Pkok9n"
      },
      "source": [
        "# creating dir for storing prediction of test images\n",
        "! mkdir test_maps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzpGsdynmWJj"
      },
      "source": [
        "test_file = [ f for f in os.listdir('test_images/') if f.endswith('.jpg') ]\n",
        "\n",
        "for i in test_file:\n",
        "    img = preprocess_images(['test_images/'+i],shape_r,shape_c)\n",
        "    img = torch.tensor(img.copy(),dtype=torch.float)\n",
        "    for idx,x in enumerate(img):\n",
        "        img[idx] = normalize(x)\n",
        "    img = img.cuda()\n",
        "    print(i)\n",
        "    pred = model.forward(img)\n",
        "    cv2.imwrite('test_maps/' + i[:-3] + 'png',pred[0].squeeze(0).data.cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pkCoXgZ06I8"
      },
      "source": [
        "# The code below is for the experimental model after removing the prior from the network and changing the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA5WPhjqaPmj"
      },
      "source": [
        "# for experimental model without the prior\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class MLNet(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(MLNet, self).__init__()\n",
        "        features = list(models.vgg16(pretrained = True).features)[:-1]\n",
        "        \n",
        "        features[23].stride = 1\n",
        "        features[23].kernel_size = 5\n",
        "        features[23].padding = 2\n",
        "                \n",
        "        self.features = nn.ModuleList(features).eval() \n",
        "        self.fddropout = nn.Dropout2d(p=0.5)\n",
        "        self.int_conv = nn.Conv2d(1280,64,kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.pre_final_conv = nn.Conv2d(64,1,kernel_size=(1, 1), stride=(1, 1) ,padding=(0, 0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        results = []\n",
        "        for ii,model in enumerate(self.features):\n",
        "            x = model(x)\n",
        "            if ii in {16,23,29}:\n",
        "                results.append(x)\n",
        "        x = torch.cat((results[0],results[1],results[2]),1) \n",
        "        x = self.fddropout(x)\n",
        "        # 64 filters convolution layer\n",
        "        x = self.int_conv(x)\n",
        "        # 1*1 convolution layer\n",
        "        x = self.pre_final_conv(x)\n",
        "        x = torch.nn.functional.relu(x,inplace=True)\n",
        "        return x\n",
        "\n",
        "# Modified MSE Loss Function\n",
        "class ModMSELoss(torch.nn.Module):\n",
        "    def __init__(self,shape_r_gt,shape_c_gt):\n",
        "        super(ModMSELoss, self).__init__()\n",
        "        self.shape_r_gt = shape_r_gt\n",
        "        self.shape_c_gt = shape_c_gt\n",
        "        \n",
        "    def forward(self, output , label ):\n",
        "        output_max = torch.max(torch.max(output,2)[0],2)[0].unsqueeze(2).unsqueeze(2).expand(output.shape[0],output.shape[1],self.shape_r_gt,self.shape_c_gt)\n",
        "        loss = torch.mean( ((output / output_max) - label)**2 / (1 - label + 0.1) )  #+  torch.sum(reg)\n",
        "        return loss\n",
        "\n",
        "# Input Images size\n",
        "shape_r = 240\n",
        "shape_c = 320\n",
        "shape_r_gt = 30\n",
        "shape_c_gt = 40\n",
        "\n",
        "last_freeze_layer = 23\n",
        "\n",
        "model = MLNet().cuda()\n",
        "for i,param in enumerate(model.parameters()):\n",
        "  if i < last_freeze_layer:\n",
        "    param.requires_grad = False\n",
        "   \n",
        "criterion = ModMSELoss(shape_r_gt,shape_c_gt).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3,weight_decay=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkQj3ELv0jkU"
      },
      "source": [
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "loss_history = []\n",
        "nb_epochs = 10\n",
        "batch_size = 16\n",
        "\n",
        "for epoch in range(nb_epochs):\n",
        "  t1 = time.time()\n",
        "  image_trained = 0\n",
        "  for i,gt_map in generator(batch_size):\n",
        "      optimizer.zero_grad()\n",
        "#       print (i.shape)\n",
        "      i,gt_map = torch.tensor(i.copy(),dtype=torch.float),torch.tensor(gt_map,dtype=torch.float)\n",
        "      for idx,x in enumerate(i):\n",
        "        i[idx] = normalize(x)   \n",
        "      i,gt_map = i.cuda(),gt_map.cuda()\n",
        "      image_trained += batch_size\n",
        "      out = model.forward(i)\n",
        "      loss = criterion(out,gt_map)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "      if image_trained % (batch_size*20) == 0:\n",
        "        print (\"Epochs:{} Images:{} Loss:{}\".format(epoch,image_trained,loss.item()) )\n",
        "  t2 = time.time()\n",
        "  time_per_epoch = (t2 - t1) / 60.0\n",
        "  print ('Time taken for epoch-{} : {}m'.format(epoch,time_per_epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cIWlWhy0k4f"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "# saving model weight\n",
        "additional_info = 'my_model'\n",
        "full_path = 'saved_models/' + str(datetime.datetime.now()) + '_' + additional_info + '.model'\n",
        "torch.save(model.state_dict(), full_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ5clLVc0tXr"
      },
      "source": [
        "PATH = # path of the saved model\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH8Rk42n1TzW"
      },
      "source": [
        "Now test on some random images from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ6HLqmP02QD"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "no_visual = 15\n",
        "visual_cnt = 0\n",
        "for i,gt_map in generator(1,phase_gen='val'):\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    org_image = i[0].copy()\n",
        "    org_image = np.rollaxis(org_image, 0, 3) \n",
        "    i = torch.tensor(i.copy(),dtype=torch.float)\n",
        "    for idx,x in enumerate(i):\n",
        "        i[idx] = normalize(x)\n",
        "    i = i.cuda()\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    ax1.set_title('original')\n",
        "    plt.imshow(org_image)\n",
        "    # ax1.plot(org_image)\n",
        "    out = model.forward(i)\n",
        "    \n",
        "    ax1 = plt.subplot(3, 3, 2)\n",
        "    ax1.set_title('predicted')\n",
        "    plt.imshow(out[0].squeeze(0).data.cpu().numpy(),cmap='gray')\n",
        "    \n",
        "    ax1 = plt.subplot(3, 3, 4)\n",
        "    ax1.set_title('gt')\n",
        "    plt.imshow(gt_map[0][0],cmap='gray')\n",
        "    \n",
        "    plt.show()\n",
        "    visual_cnt += 1\n",
        "    if visual_cnt > no_visual:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYvc4lI16q1w"
      },
      "source": [
        "## Below code is to run model on video. You have to run this on your pc to get the results as colab don't support this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ritoJZoC6jA-"
      },
      "source": [
        "full_path = 'saved_models/' + '2019-11-21 10_43_21.727494_my_model' + '.model'\n",
        "prior_size = (int(shape_r_gt / 10) , int(shape_c_gt / 10))\n",
        "model = MLNet(prior_size).cuda()\n",
        "model.load_state_dict(torch.load(full_path))\n",
        "\n",
        "\n",
        "# Check if camera opened successfully\n",
        "if (cap.isOpened()== False): \n",
        "  print(\"Error opening video stream or file\")\n",
        "\n",
        "\n",
        "def rescale_frame(frame, percent=75):\n",
        "    width = int(frame.shape[1] * percent/ 100)\n",
        "    height = int(frame.shape[0] * percent/ 100)\n",
        "    dim = (width, height)\n",
        "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "# Read until video is completed\n",
        "while(cap.isOpened()):\n",
        "  # Capture frame-by-frame\n",
        "    ret, img2 = cap.read()\n",
        "    img = img2\n",
        "    if ret == True:\n",
        "    \n",
        "        img = preprocess_images(img,shape_r,shape_c)\n",
        "        img = torch.tensor(img.copy(),dtype=torch.float)\n",
        "        for idx,x in enumerate(img):\n",
        "            img[idx] = normalize(x)\n",
        "        img = img.cuda()\n",
        "        \n",
        "        pred = model.forward(img)\n",
        "        #    cv2.imwrite('test_maps/' + i[:-3] + 'png',pred[0].squeeze(0).data.cpu().numpy())\n",
        "            # Display the resulting frame\n",
        "        img = pred[0].squeeze(0).data.cpu().numpy()\n",
        "        img = rescale_frame(img, percent=400)\n",
        "        img = cv2.resize(img, (img.shape[1]*4, img.shape[0]*4))\n",
        "        cv2.imshow('Frame', img)\n",
        "        cv2.imshow('frame2', img2)\n",
        "            # Press Q on keyboard to  exit\n",
        "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "\t\t\t      break\n",
        "        \n",
        "  # Break the loop\n",
        "    else: \n",
        "        break\n",
        "  \n",
        "# When everything done, release the video capture object\n",
        "cap.release()\n",
        " \n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}